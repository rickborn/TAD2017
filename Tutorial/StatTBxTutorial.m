% StatisticsTBx.m%% Randy Gallistel's introduction to the capabilities of MATLAB's Statistics% Toolbox. It also serves as a good review of some basic MATLAB functions.%% RTB downloaded it on 08 December 2017 from his website:% http://ruccs.rutgers.edu/gallistel-research-interests%% RTB updated functions and cleaned up text, 09 December 2017% RTB added front-end mini-tutorial on MATLAB, June 2018%% RTB mini-MATLAB tutorial% For those desiring a more basic re-introduction to MATLAB, there is a% free, on-line, self-paced tutorial from the Mathworks at:% https://matlabacademy.mathworks.com/% Select "Launch" for the "MATLAB Onramp" course%% Note that you will need to sign in to your "Mathworks Account" to access% the tutorial. If you don't have one, it is easy to create one (and free).% Once you have created an account, if you associate it with Harvard's% MATLAB site license (596681), you will have access to everything,% including a very nice, complete on-line version of MATLAB. In order to% associate your account with the Harvard site license, you MUST use your% official Harvard e-mail address to open your Mathworks account.% MATLAB on-line: https://matlab.mathworks.com/%% Indexing into arrays% Most of the data you will deal with in TAD will be stored in arrays% (a.k.a. matrices). There are two numbers associated with every element in% an array: 1) the location in the array, called the "address" or, more% commonly, the "index" and 2) the value stored at that location.% Let's create a 2-dimensional array:A = [2,7,4;8,6,1;3,9,4];    % a 3-by-3 array% If you want to look at the array, you can either just type 'A' at the% MATLAB command line (i.e. after the '>>' in the Command Window), or you% can type 'disp(A)' at the command line (or in a script).% The 'size' of the array is the number of its rows and columns:[nRows,nColumns] = size(A);% The 'length' of an array is the size of its longest dimension. In this% case, there are 3 rows and 3 columns, so the length is 3. If our array% had 5 rows and 3 colums, the length would be 5.length(A)% The number of elements in an array is the product of it number of rows% and columns, which we can calculate in one of two ways:%% 1) The 'prod' function multiplies together all the elements passed to it:% e.g. >> prod([2,3,4]) = 24% Question: What would you expect the output to be for 'prod(A)'?nElements1 = prod(size(A));% 2) The 'numel' command"nElements2 = numel(A);% NOTE: If you look above, you will see that the 'prod' command is% highlighted by an orange box. See what happens if you move the cursor and% let it hover over that orange box.% There are 3 basic ways to specify locations in an array, usually referred% to as "indexing into an array":% Method 1: row,column% If we want the value in the 3rd row, 2nd column we type:A(3,2)% Note that we always use parentheses to index into an array. Also note% that we always specify the row 1st and the column 2nd. This is just% MATLAB's convention: row 1st, column 2nd. If we had a 3-dimensional% array, which you can think of as a stack of 2D arrays, we would specify% the 3rd dimension, or 'page', as the 3rd number. This can go on forever:% the dimensionality of an array is just the number of indices you need to% specify to get at an element in that array.% Method 2: linear indexing% If we enter a single number as the index into an array, MATLAB will% concatenate all of the columns, i.e. as if we had used the colon operator% to linearize the array: A(:), and simply count down to the single index.% So, for example, to access the element at A(3,2) with a single index, we% would use:A(6)% Method 3: logical indexing% This is the most powerful and counter-intuitive way. We create an array,% let's name it 'L' for now, of type 'logical' (meaning it consists of only% 1's and 0's) the same size as the array into which we want to index,% putting a 1 where we want to 'see' (or obtain) the value and 0's% elsewhere. You can imagine that L is like a screen that is opaque where% there is a 0 and clear where there is a 1. When we use it to index into% array, A, it is as if we overlaid the logical screen on array A so that% we 'see' only the locations that are clear (=1) in L. So, to get the same% element in row 3, column 2, we can manually create our indexing array:L = logical([0,0,0;0,0,0;0,1,0]);% Now, and this is the part that feels weird, we just use 'L' to index% directly into 'A':A(L)% We could also specify 'L' by first creating an array of all 0's and then% just changing the one element to a 1:L = logical(zeros(3,3));L(3,2) = 1;% Note that we had to use the casting command 'logical' to change the array% type of 'L' from 'double' (the default) to 'logical' in order to use it% as an indexing array. A more straightforward way is to use the 'false'% and 'true' functions to create 'L':L = false(3,3);L(3,2) = true;A(L)% This seems like a lot of effort to go to in order to grab one element% from an array. The real power of this method is that we can use a% logical statement to screen for elements meeting certain criteria. Let's% say we wanted to know all of the locations in array 'A' where the number% stored was greater-than-or-equal-to 5:L = A >= 5% The logical command ('>=' in this case) will always return an array of 0s% and 1s that is the same size as the array we've queried. In this case, we% get a 3x3 array that consists of 1's where the condition is met and 0's% elsewhere. The cool thing is that we can then use this logical array, L,% to index into our array, A. This will return only the values in A where% there is a 1 in the corresponding location in L.A(L)% The powerful part is that L can be any combination of 0s and 1s, so that% it gives us enormous flexibility in selecting locations in an array.% Imagine that we wanted to know all of the values in A that were, say,% less than or equal to 4. We simply perform the appropriate logical% operation:L = A <= 4% Then use L to index into A as before:A(L)% Of course, we can do it all in one step, without the need to create the% intermediate variable, L:A(A <= 4)% This looks strange, but once you become proficient at MATLAB, you will% find yourself using constructions like this all the time.%% Random sampling with indexing% In this course, we will do a lot of so-called 'resampling', that is,% taking random samples from real data. We will do this both 'with% replacement' (bootstrap) and 'without replacement' (permutation test). To% do this, we simply generate a random string of indices of the appropriate% size, and use this to "grab" the samples from the original data. Because% we are generating indices, we need to use integers from 1 to n, and% because we want to sample uniformly, we want each integer from 1 to n to% have an equal probability of being chosen. The distribution that fulfills% these criteria is called the "uniform discrete random distribution," and% the corresponding MATLAB function is 'unidrnd'. If we type 'unidrnd(6)'% at the command line, the function returns a single number from 1 to 6% (inclusive) with equal probability. It is the equivalent of rolling a% single, fair die. Try it:unidrnd(6)% If we want to simulate rolling 6 dice and observing the results, we can do% this by telling 'unidrnd' to return an m x n array of values. We first% specify the range, then the size of the sample array:unidrnd(6,1,6)% Run this same command several times (use the 'up' arrow key to repeat the% last command) and see what you get.% To see if the function is really returning the numbers 1,2,3,4,5,6 with% equal probability, we can roll many, many dice and plot a histogram:manyDice = unidrnd(6,1,10000);binCenters = 1:6;figure, hist(manyDice,binCenters);  % pretty flat, as one would hope% Now here is the cool part. Suppose we have some data stored in an array,% 'X', consisting of 6 values:X = [246.2611  326.1952  237.1099  128.2323  377.5852  141.2841];% If we want to draw a new sample of size 6 by randomly sampling from 'X'% WITH REPLACEMENT (i.e. We can draw the same sample more than once), then% we just use the output of 'unidrnd' to index into array 'X':newSample = X(unidrnd(6,1,6));% This seems weird, perhaps, but it is the essence of The Bootstrap. Much% more on this topic will follow.%% Example: Bootstrap for SEM using 'unidrnd'% load in some sample dataload cellBodyArea% The file 'cellBodyArea.mat' contains two variables, 'FF' and 'FB', each% of which contains measurements from 50 neurons of two different anatomical% types.% Calculate the standard error of the mean using the well-known formula:nSamples = length(FF);semFF = std(FF) / sqrt(nSamples);% What does the SEM tell us? Literally, it is the standard deviation of the% sampling distribution of the mean. But what does this mean. It means that% if we went back to our original population, repeated our same experiment,% got a new mean from our new sample, and repeated this many, many times we% would get a distribution of means. This is the sampling distribution of% the mean. And, by definition, the SEM is the standard deviation of this% distribution. But, of course, repeating our experiment thousands of times% is not practical. nBoot = 100000;allMeans = zeros(nBoot,1);rng defaultfor k = 1:nBoot    allMeans(k) = mean(FF(unidrnd(nSamples,nSamples,1)));endsemBootFF = std(allMeans);% Compare the two values of SEM%% 'Shuffling' data using 'randperm'% For the bootstrap, we sample WITH REPLACEMENT, but there are other times% when we want to sample WITHOUT REPLACEMENT. That is, we want to use ALL% of the original data for each new sample, we just want to scramble it% first, as one does when one shuffles a deck of cards. We usually do this% to break some association we are interested in testing under H0 (see% example below). Again, the trick is to generate a random list of whole% numbers from 1 to n (where 'n' is the size of our original data set), and% use these to index into our data array. In this case, we just want a% random ordering of all the numbers from 1 to n, with each number% appearing only once. The MATLAB function that accomplishes this is called% 'randperm'. Try it:randperm(6)% Now here is the cool part. Suppose we have some data stored in an array,% 'X', consisting of 6 values:X = [246.2611  326.1952  237.1099  128.2323  377.5852  141.2841];% If we want to draw a new sample of size 6 by randomly sampling from 'X'% WITHOUT REPLACEMENT (i.e. We will use each sample exactly once), then% we just use the output of 'randperm' to index into array 'X':newSample = X(randperm(6));%% Permutation test of H0 using 'randperm'% What is the actual difference measured from our data? We could measure% any statistic we want here: the difference in the means, the difference% in the medians, the variances, the skewness . . . anything we might be% interested in.realMeanDiff = abs(mean(FB) - mean(FF));% This is the simple, but key, conceptual step. We pool all of the data.% Then, on each iteration of the 'for' loop below, we shuffle the data and% arbitraily assign half of it to be of type 'FB' and the other half 'FF'.% In this way, we are breaking the true association between the measurments% and the cell types.H0data = [FB;FF];nTotal = length(H0data);nSamples = length(FF);nPerm = 100000;allMeanDiffs = zeros(nPerm,1);rng defaultfor k = 1:nPerm    shuffledData = H0data(randperm(nTotal));    FBperm = shuffledData(1:nSamples);    FFperm = shuffledData(nSamples+1:end);    allMeanDiffs(k) = mean(FBperm) - mean(FFperm);endpVal = sum(allMeanDiffs > realMeanDiff) / nPerm;% Compare this p-value with that from a 2-sample t-Test:[h,p] = ttest2(FB,FF,'tail','right');%% Random sampling using 'datasample'% As often happens, when a practice becomes widespread, as bootstrapping% has, eventually the MATLAB toolboxes catch up and include fancier% functions that take care of things in a nicer way. This is the case with% re-sampling from data, aided by the 'datasample' function. To learn more% about it, type 'doc datasample' at the command line.% Data sample allows us to specify whether we want to sample with or% without replacement, as well as to specifiy which dimension we want to% sample from. So, sample WITH REPLACEMENT, we can either use:rng defaultnewSample = X(unidrnd(6,1,6));% OR:rng defaultnewSample = datasample(X,6,'Replace',true);% And to sample WITHOUT REPLACEMENT, we use eithernewSample = X(randperm(6));% OR:newSample = datasample(X,6,'Replace',false);% 'datasample' has some other nice features, such as allowing us to% specify probabilities that are not uniform. For example, this line of% code will generate a random DNA sequence where 'C' and 'G' are more% likely to occur than 'A' and 'T'seq = datasample('ACGT',48,'Weights',[0.15,0.35,0.35,0.15]);% NOTE: The command 'rng default' sets MATLAB's random number generator to% it default value. This means that calls to functions like 'unidrnd' will% generate the exact same sequence of random numbers. Try it:rng defaultrandomArray1 = unidrnd(6,5,6);rng defaultrandomArray2 = unidrnd(6,5,6);% This is useful for class exercises where we want to get a single% 'correct' answer for, say, a bootstrapped SEM. If everyone in the class% sets the random number generator to its default value prior to running% their re-sampling algorithm, we will all get the same answer (also% provided we do the same number of bootstrap iterations).% At this point you might be saying to yourself, "Hey, wait a minute.% That's not very random!" And you would be right. Technically speaking,% these numbers are "pseudorandom" meaning that they are produced by an% algorithm operating on a 'seed' value; they have all the right properties% for something that is truly random (What would these be?), but if the% algorithm starts with the same seed, it will generate the same sequence.% You can read more about this at:% https://en.wikipedia.org/wiki/Random_number_generation%% 'for' loops and simulations% Imagine we flip a coin 10 times and get 7 heads. We want to know how% likely it would be to get 7 or more heads if we repeated our 10-flip% experiment a large number of times using a 'fair' coin. To do this, we% make use of MATLAB's ability to generate random numbers:% To simulate a single coin toss, we can use:thisFlip = round(rand);% To make things simple, we will arbitrarily assign the value of '1' to% heads and the value of '0' to tails.% Run this line of code several times and convince yourself that it is% doing what we want it to do.% The first thing we want to do is to simulate a single 'experiment' in% which we flip the coin 10 times:nFlipsPerExperiment = 10;   % What we actually didnHeads = 7;                 % The result we actually got% Create a place to store the results of each flip:thisExperiment = zeros(nFlipsPerExperiment,1);for k = 1:nFlipsPerExperiment    thisFlip = round(rand);    thisExperiment(k,1) = thisFlip;end% How many times will this 'for' loop execute? What will 'thisExperiment'% look like at the end?% In order to get a probability, we need to repeat our experiment many,% many times, let's say 10,000. So we *nest* our 'for' loop for each% experiment within a 2nd 'for' loop for each simulation. At the end of% each experiment, we will tally up the number of heads ( = 1's) and store% the result:nSims = 10000;nHeadsPerExperiment = zeros(nSims,1);for j = 1:nSims    thisExperiment = zeros(nFlipsPerExperiment,1);        for k = 1:nFlipsPerExperiment        thisFlip = round(rand);        thisExperiment(k,1) = thisFlip;    end        nHeadsPerExperiment(j,1) = sum(thisExperiment);end% Now we want to know for how many of our simulations we got a value% greater than or equal to the result we actually got:nSuccesses = nHeadsPerExperiment >= nHeads;% Finally, to calculate a probability, we divide by the total # of% experiments we simlulated:probNSuccesses = sum(nSuccesses) / nSims;%% Simulations without using 'for' loops% MATLAB is 'vectorized' meaning that it can automatically perform% operations on arrays. This means that for certain simulations, we don't% need to use 'for' loops (which are slow). In this case, we can generate% an array where each colum is an experiment and each row is a coin flip.allFlips = round(rand(nFlipsPerExperiment,nSims));% So we have an array of 0s and 1s, and we can think of each column as one% experiment of 10 coin flips. Now we want to know, for each experiment,% how many heads did we get. We use 'sum' which will automatically operate% across rows (i.e. down each column), so that we will get 10,000 values of% the # of heads for each experiment.nHeadsPerExperiment = sum(allFlips);% Now we want to know how many of our sums were greater than or equal to 7:nSuccesses = nHeadsPerExperiment >= nHeads;probNSuccesses = sum(nSuccesses) / nSims;%% Direct calcualtion of binomial probabilities% Of course, we can use the binomial PDF to calculate the precise answer.% We simply sum up the proability of getting 7,8,9 or 10 heads on 10 flips,% assuming a fair coin:sum(binopdf([7:10],10,0.5))% We can do the same calculation with the CDF. But remember that the CDF% tells us the probability of getting a value less than or equal to the% reference. But since all of the probabilities must sum to one, we can% find the cumulative probability of getting 0,1,2,3,4,5 or 6 heads, and% then subtract this sum from 1.1 - binocdf(6,10,0.5)% Are our 3 different answers consistent?%% 'hist' and 'histogram' for looking at distributions:% In our above simulation, we calculated the number of heads we got from% each of 10,000 experiments, with each experiment consisting of 10 flips.% It's often very useful to look at how often each value occurred. This is% a histogram that we plot using the 'hist' function. In this case, we know% beforehand all of the possible values: 0 through 10. So we can tell% 'hist' to bin in the most intuitive way:binCenters = 0:10;figuresubplot(2,1,1);hist(nHeadsPerExperiment,binCenters)% Always label the axes:xlabel('# of heads from 10 flips');ylabel('# of experiments');% The histogram matches our intution: for a fair coin, we would expect to% get 5 heads from 10 flips most often, with other numbers falling off to% either side. Getting 0 or 10 heads should be rare.% We can convert our histogram to a probability distribution by normalizing% each bin by the total number of experiments. This will force the sum of% all the different possibilities to be 1 as it must be. But first we need% to know how many experiments there are in each bin. We can use 'hist' to% do the counting for us:nHeadsPerBin = hist(nHeadsPerExperiment,binCenters);% Now we normalizepHeadsPerBin = nHeadsPerBin ./ sum(nHeadsPerBin);% And now, since we've already done the binning, we use 'bar' to plot the% histogram:subplot(2,1,2);bar(binCenters,pHeadsPerBin);xlabel('# of heads from 10 flips');ylabel('Probability');% NOTE: MATLAB currently recommends that you use the newer function% 'histogram' instead of 'hist'. The newer function is more% sophisticated--for example, it uses a heuristic to automatically% determine the appropriate number of bins (instead of defaulting to 10% bins, as is done by 'hist'). There are some other nice features of% 'histogram'. For example, in order to plot interleaved histograms from% different categories, 'hist' required an array with one column for each% category, which meant that you had to have the same number of% observations for each histogram, which often meant having to pad shorter% columns with NaN's. 'histogram' eliminates this problem. All you do is% plot the first category using 'histogram', then set 'hold on', then% simply plot the next category with 'histogram'. Also, 'histogram' returns% a histogram object that contains a bunch of data. For example, if you% wanted to get the counts in each bin, you'd use:%% >> h = histogram(X);% >> binCounts = h.BinCounts;%% One thing that I find annoying about 'histogram' (or it's companion,% 'histcounts', for binning and counting without plotting), is that it does% not allow you to specify Bin *Centers* but only Bin Edges. So, in the% above example, if we had tried to use 'histcounts' to do our binning% based on the bin centers (0:10), we would get an error telling us that "X% must be the same length as Y." In order to use 'histcounts' we would need% to specify the edges as:%% binEdges = -0.5:1:10.5;% nHPB = histcounts(nHeadsPerExperiment,binEdges);% figure, bar(binCenters,nHPB);%% Or we could tell 'histogram' to center the bins on integers:figureh = histogram(nHeadsPerExperiment,'BinMethod','integers');% Finally, note that we can do fancier stuff using 'histogram'. For% example, we can tell it to normalize the data in a variety of ways. If we% want to see the cumulative density function, we just say:figureh = histogram(nHeadsPerExperiment,'BinMethod','integers','Normalization','cdf');% Note also that you can modify your histogram without have to make a new% call to 'histogram' just by changing the Property Values in 'h'. For% example, if we type:h.NumBins = 6;% we see the histogram in our figure change% For old-timers, see the MATLAB documentation on:% "Replace Discouraged Instances of hist and histc"%% A brief re-cap% The operations we covered above comprise the vast majority of what you% will need to know for this course. We will get to some fancier stuff,% such as using 'glmfit' to fit regression models, but this will be covered% in detail when we get there. So here, again, is what you should come to% the course feeling comfortable with:%% 1) indexing into arrays% 2) using 'for' loops to simulate experiments% 3) using logical statements, such as '>=' and '=='% 4) getting random numbers: 'rand','unidrnd','randperm'% 5) plotting the results of simulations using 'hist' and 'bar'%% The Gallistel statistics toolbox tutorial begins here:% The tutotial begins by describing what you can do with the built-in% distribution functions. Distribution functions are at the heart of% statistics and probability. There are 20 probability distributions% included with the toolbox and each of them supports 6 functions: pdf,% cdf, inverse cdf, random sample generation, mean and variance as function% of parameters, and parameter estimates for data.%% For the binomial distribution, the corresponding functions are:%% 1. PDF: binopdf% 2. CDF: binocdf% 3. Inverse CDF: binoinv% 4. Random sample generation: binornd% 5. Mean and variance: binostat% 6. Parameter estimates: binofit%% The naming convention is such that you generally take an abbreviated% distribution name (e.g. 'normal'->'norm', 'Poisson'->'poiss') and use the% same suffix. So to get random numbers from a Poisson distribution, the% function name is 'poissrnd'.%% Each section beginning with a '%%' is a 'cell' (in MATLAB speak). You can% execute a given section by mouse-clicking inside (the block should turn% yellow) and then simultaneously hitting the 'ctrl' and 'enter' keys (PC)% or the 'command' and 'enter' keys (Mac).%% Binomial Distribution: % the distribution of the number of heads (successes) that you% get when you flip a (biased) coin N number of times. The parameters are% the bias (probability of heads) and the number of times it is flippedX=(0:20)';% Creating a column vector of 21 possible outcomes, identified by the% integers 0 to 20. This vector contains only integers, because discrete% distributions are only defined for discrete possible outcomes, that is,% there are only a countable number of possibilities. Recall that the% default behavior is to increment by 1. Other increments must be% specified. For example, if we wanted only even integers from 0 to 20:% X = (0:2:20)';    % column vector% X = 0:2:20;       % row vectorY=binopdf(X,5,0.8); % Generates the binomial distribution function for the case in which one% flips the coin 5 times and there is a 0.8 probability of getting a head% on any one flip. There are six possible outcomes: 0 heads, 1 head, 2% heads, 3 heads, 4 heads, or 5 heads. The vector Y (the vector containing% the results from calling this command) gives the probability of each of% these outcomes. Although this is called a pdf, the values it delivers are% in fact probabilities (rather than probability densities), because there% are only a discrete number of possible outcomes. The sum of the% probabilities associated with each possible outcome is always one.% Sometimes the probabilities delivered by a discrete pdf are called% proability masses.%% Plotting the distributionsubplot(2,1,1); % This command opens a figure window [Figure(1)] and tells MATLAB that in% this figure window there will be 2 rows and 1 column of panels (in other% words, two  panels, one beneath the other), and, finally, it activates% the first of the two panels. Subsequent plot commands will "plot into"% that panel (create the commanded graph within that panelplot(X(1:7),Y(1:7));% Makes a conventional line plot of the probabilities stored in the vector% Y. This is not a good way to plot discrete probability distributions,% because the lines connecting successive values imply that there are% intermediate possibilities (that is, outcomes between, for example, 1 and% 2) and, moreover, that those intermediate possibilities have% probabilities. In fact, of course, there are no intermediate% possibilities-every possibility has been given an integer, and these% non-existent intermediate possibilities have, of course, no probabilities% associated with themxlim([0 7]) % Defines the range of values on the X axis. I am doing this because I want% the X axes of my two plots to agree, and the autoscaling function, which% sets the x limits when you call the graph command is somewhat% unpredictable in the limits that it sets% ALWAYS label your axes:xlabel('Number of Successes')ylabel('Probability of N');% Put a title on the graph:title('Not a good way to plot a discrete distribution');subplot(2,1,2)% Makes the second panel active. Hereafter, the graphs drawn by our plot% commands will go into that panelbar(X(1:7),Y(1:7),1)% Draws a bar graph of the probabilities in the vector Y(1:7) for the% outcomes (categories) in the vector X(1:7). The 1 in the third argument% makes the bars occupy 100% of the interval between bars. Generally% speaking discrete probability distributions should be plotted as bar% graphs.xlabel('Number of Successes')ylabel('Probability of N');title('Discrete distributions should be plotted as histograms')%% Discrete and Cumulative Binomial Probability Functionsfigure % Opens a second figure window [Figure(2)]. Hereafter subplot commands will% create panels within this new window (new figure)subplot(2,1,1) % Top panel of new figurebar(X(1:7),Y(1:7),1) % Plotting the discrete probabilities in the vector Y, just as before.xlabel('N'); ylabel('Probability of N');hold on % This tells MATLAB that when we issue the next graph-making command we% want it to keep the graph it has already made and add the graph(s)% commanded by the (those) later command(s) to the one already plottedstairs(X,cumsum(Y),'r');% The command cumsum(Y) computes a vector that is the running sum of the% values in the vector Y. By embedding this command inside a plot command,% we create the graph of this running (or cumulative) sum. Here we use a% special plot command created specifically for plotting cumulative% discrete distributions. If you use the ordinary plot command to plot a% cumulative discrete distribution, it will draw straight lines between the% successive values of the running sum. These lines imply that the% cumulative probability increases continuously over the "intervals"% spanned by these lines, but, these "intervals" are a fiction; they don't% exist, and, a fortiori, there is no increase in probability associated% with them. The stairs function plots these graphs the way they should be% plotted, that is, as a sequence of step increases in the cumulative% probability. Discrete cumulative probability functions should always be% plotted using stairs, to make it clear that they take on only integer% values. Empirical cumulative distribution functions should also be% plotted using stairs, unless they contain a large number of observations% (say >100). The second argument of the stairs function ('r') tells it to% make the line red. Because we have used 'hold on', the line plot of the% cumulative distribution is superimposed on the bar plot of the discrete% probabilities% Add a legend to our plot to explain the different lineslegend('PDF', 'CDF');%% Adding some more cumulative binomial distribution functionshold off % We take the hold off, because we want to get rid of the bar plot of the% discrete probabilities and just look at several different cumulative% probability distributions plotted on the same axes. One of the many% advantages of cumulative distribution functions is that you can plot% several of them on the same axes without creating an unintelligible mess.stairs(X,cumsum(Y),'r');hold onxlabel('N'); ylabel('Probability of N');% This series of commands recreates the cumulative distribution plot that% we have already looked at.stairs(X,binocdf(X,7,0.2),'g');% Now we add to the previous distribution a new one. This is the% distribution that we get when there are 7 flips of the coin on each% trial, with only a 0.2 probability of a heads on any given flip. The 'g'% tells MATLAB to make the line for this distribution green.stairs(X,binocdf(X,20,0.4),'b');% Now we add a third cumulative distribution, the that we get when each% trial consists of 20 flips and there is a 0.4 probability of a heads on% any one flip. The line for this distribution is blue ('b')stairs(X,binocdf(X,20,0.7),'k');% Now we add a fourth cumulative distribution, the that we get when each% trial consists of 20 flips and there is a 0.7 probability of a heads on% any one flip. The line for this distribution is black ('k')legend('N=5, p=0.8','N=7, p=0.2','N=20, p=0.4','N=20, p=0.7','Location','SouthEast');title('Binomial Cumulative Density Functions');%% Inverse binomial cdf% Inverse cumulative distribution functions take cumulative probabilities% as inputs, and give the X values associated with those cumulative% probabilities. Most commonly, the probabilities given as inputs are alpha% levels (significance levels), one wants to know what value X has to have% in order for there to be less than alpha probability of observing a value% of X that extreme or more extreme.p = (0.01:0.01:1)';% Creates a column vector of p values in order to show the inverse cdfsubplot(2,1,2);% Activates the lower panel in Figure(2)stairs(p,binoinv(p,20,0.4),'b'); % Plots the inverse cumulative probability function for the distribution% obtained when each trial consists of 20 flips, with a 0.4 probability of% heads on any flip. What you can see in this distribution is that the% probability of getting 4 or fewer heads is 0.06, while the probability of% getting 3 or fewer is 0.02, and the probability of getting 1 or fewer is% too small to be read accurately off of this graph. Thus, if you got only% 3 heads, you could say that there was only one chance in 50 of getting so% few heads if the head's bias on the coin was really 0.4 (or greater)% The inverse distribution gives the N such that the probability of getting% an N that small is less than p.title('Inverse binomial CDF');xlabel('Probability of Getting N or Fewer');ylabel('N')legend('N=20, p=0.4')% The graph of the inverse distribution is the same as the one you see if% you take the graph of the cdf, turn it over (reflect it) and rotate it 90% degrees clockwise so that what was the y axis is now the x axis (seen% dimly through the paper).%% Generating random samples from a binomial distribution.% So far, we have been plotting the true probabilities of the various% possible outcomes, as computed from first principles. Of course when you% actually do the trials some finite number of times and record the% frequencies of the various outomes, those frequencies are never (well,% rarely) EXACTLY what they should be. It is incredibly useful and% instructive to have a means of "actually doing" the trials some% reasonable number of times and seeing what "real data" look like. The% data generated in this way are invaluable in testing ideas and models,% and in getting a gut feel for probabilities and how they behave. Of% course these data are not "real data", they are simulated data, but they% are a good stand in for real data in forming your feel for what real data% should look like if they are in fact generated by a binomial process.Rb = binornd(20,0.4,30,1);% Generates a column vector of 30 random samples from a binomial% distribution consisting of counts, where each count is the number of% heads generated by flipping 20 times a coin that has a 0.4 probability of% coming up heads. If you repeat this command, it will give you a different% string of numbers, just as you would get a different sequence of outcomes% (and different frequencies of the different possible outcomes) if you ran% another 30 trials. However, if you first type 'rng defuault', you will% get the same answer every time. You could achieve a similar result by% shutting down MATLAB, rebooting it and running the command again, because% MATLAB sets its random number generator to the same initial state% whenever it is booted.FrqN = hist(Rb,X);% The frequencies of different heads counts (possible outcomes). Note that% when we call 'hist' with a return value, it does not draw a histogram,% but rather just bins the data (in this case, it uses the bin centers% provided in X) and returns the counts in each bin. If we wanted to use% this to then plot a histogram, we would use 'bar(X,FrqN)'CmFrq = cumsum(FrqN) ./ sum(FrqN);% Empirical cumulative probability function. figure(2);subplot(2,1,1) % Going back to the top panel of 2nd figure% Plotting the empirical CDF on same graph as the theoretical CDFstairs(CmFrq,'b--');hLeg = legend('N=5, p=0.8','N=7, p=0.2','N=20, p=0.4','N=20, p=0.7','Empirical N=20,p=0.4');set(hLeg,'Location','SouthEast');% This illustrates MATLAB's 'handle graphics' feature. Every function that% draws anything will return a 'handle', which is merely a pointer to that% graphics object. You can then use that handle to modify any attribute of% the graphics object. To see the names of the attributes that can be% modified, type 'get(handle)'. So, for example, to see the attributes of% the legend command, type 'get(hLeg)'.%% Geometric Distribution: % The distribution of the number of flips PRIOR TO the first heads when% flipping a (biased) coin. The only parameter is the bias. The geometric% distribution is the discrete approximation to the exponential% distribution. When computer programs generate an exponential% distribution, what they in fact generate is a geometric approximation to% it. If the coin is flipped once each second, then the expected time to% the first heads is 1/p, where p is the probability of a heads on any% flip. The lower this probability, the longer it takes on average to get% to the first heads.X = (0:30)';% X is the column vector of possible outcomes. There are a (countably)% infinite number of possible outcomes because, in principle, it could take% an arbitrariliy large number of flips to get the first heads. In% practice, of course, it never takes anywhere near an infinite number,% but, with the probability of a heads set to only 0.1, there is a% non-trivial probability of more than 30 flips without a heads. Thus, this% distribution is truncated (censored).Y = geopdf(X,0.1); % Computes the probabilities for each of the first 31 possible outcomes% (because X is the integers 0 to 30). These values are easy to appreciate% intuitively. If we start with the value of X=0, it is just the% probability of getting a head on the first flip, which is 0.1. For X=1,% it is the probability of not getting a head on the first flip (0.9) and% then getting a head on the 2nd flip (0.1), which, because our flips are% independent Bernoulli trials, is just 0.9*0.1 = 0.09. The sequence for% X=2 is just 0.9*0.9*0.1 = 0.081. And so on.figure % Opens a third figure window [Figure(3)]bar(X,Y,1) % Bar plot of the probabilitiesxlim([0 31])xlabel('Number of Flips Prior to First Heads')ylabel('Probability')hold on % Keeps the bar plot when we add the stair plotstairs(X,geocdf(X,0.1));% Plots the cumulative geometric distribution, superimposing it on the% (simple) distributionlegend('geometric PDF','geometric CDF')title('The Geometric Distribution')%% Poisson Distribution: % The number of times a random event occurs in a given amount of time. It% is a one parameter distribution, because the variance is equal to the% mean (expected number of events). Sometimes called the distribution of% rare events because, when the expectation is large, it becomes% essentially equal to the normal distribution. Thus, this distribution is% only interesting when its expectation is small.Y = poisspdf(X,2.5); % Y contains the probabilities of observing 0, 1, 2, 3, etc events from a% random rate process when the expected number of events for whatever unit% of time one has chosen is 2.5. The expected number of events is 1/lambda% where lambda is the rate parameter (number of events per unit of time).% Thus, the expected number of events does not have to be, and generally is% not itself an integer. It can have any value greater than 0. This is% nonetheless a discrete distribution, because although 2.5 may be the% expected number of events (that is the average number), on any one trial% one cannot observe 2.5 events, one can only observe integer numbers of% events. As before, the range of likely outcomes (numbers of observed% events) is in the vector X. The total number of possible outcomes is% (countably) infinite, because in principle we could get an arbitrarily% large number of events even though we only expect to get on average 2.5% events. In practice, however, we never do observe a really large number% of events, because the odds are overwhelmingly against such an% observationfigure % Opens a fourth figure window [Figure(4)]bar(X,Y,1),xlim([0 31]); hold onxlabel('Event Count');ylabel('Probability')stairs(X,poisscdf(X,2.5),'k') % Superimposing the cumulative Poisson distributiontitle('Poisson pdf and cdf when expected number of events is 2.5')legend('PDF','CDF')%% Normal (Gaussian) distributionX = (.01:.01:1)';% For the first time, the possible outcomes have non-integer values.% Because this is a continuous distribution, there are an (uncountably)% infinite number of possibilities. Any outcome (measurement, dataum)% between minus and plus infinity is possible. Of course, we cannot create% a vector with an uncountably infinite number of values. We must create% one that samples the numbers within the likely range of outcomes. An% immediate consequence of the fact that there are an uncountable infinity% of possible outcomes between any two actually specified outcomes is that% there cannot be a finite probability associated with any single outcome.% If single outcomes had finite probabilities associated with them, then% the sum of those uncountably infinite finite probabilities would itself% be infinite, which is preposterous, because the probabilities associated% with all possible outcomes must sum to 1. That is why when we shift to% continuous distributions, what the pdf function gives us is no longer% probabilities but rather probability densities, about whose properties we% will learn more as we go.Y = normpdf(X,0.7,0.1); % Computes the probability densities associated with each of the outcomes% in our X vector. THE VALUES THIS--AND ALL OTHER--CONTINUOUS PDF FUNCTIONS% COMPUTE ARE NOT(!!) PROBABILITIES; THEY ARE PROBABILITY DENSITIES. In% other contexts, these same values are likelihoods. The difference between% a likelihood and a probability density is explained later. The principal% use of likelihoods is in finding maximum likelihood estimates of model% parameters.figure % Opens a fifth figure window [Figure(5)]plot(X,Y)% Plotting the probability density function. Notice that, for the first% time, we use the ordinary plotting command, rather than the bar command.% The ordinary plot command draws straight lines between successive data% points, implying that there are possible outcomes within the intervals% spanned by those lines, and that those outcomes have probability% densities associated with them. Now that we have shifted to the% continuous probability density functions (rather than the discrete), this% assumption is true.xlabel('X');ylabel('Probability Density')title('Notice that the peak probability density is greater than 1!')% Making a yy plot that plots the probability densities against the% left axis and the cumulative probability against the right axis. Notice,% that the cumulative probability goes to 1, as it did when we were% cumulating discrete probabilities. The cumulative probability is the% probability that the outcome was less than the value on the x axis; as% that value goes to infinity, it becomes certain that the outcome was less% than that value, because every (finite) outcome is less than plus% infinity.figureyyaxis leftplot(X,Y);hold onxlabel('X'), ylabel('Probability density');ax = axis;% plotting a vertical dashed line at the peak of the pdfline([.7 .7],[ax(3) ax(4)],'Color','k','LineStyle','--');yyaxis rightplot(X,normcdf(X,0.7,0.1));ylabel('Cumulative probability');title('The PDF is the SLOPE (derivative) of the CDF');% The probability density at any given value of X is the slope of the% cumulative probability function at that value (its derivative). Slope is% the gain in probability mass (delta probability) per unit of X. Thus, at% the point where the dashed vertical line cuts the cumulative probability% function, that function is gaining 4 units of probability mass per unit% increase in X. If it were to continue gaining probability at that rate% over a whole unit of X, the cumulative probability would have a value% greater than 4, which is, of course, impossible. But the cumulative plot% does not continue to gain at that rate; the rate falls off rapidly,% rapidly enough so that the total gain in probability is 1.% The fact that probability density is the gain in probability mass per% unit increase in X means that probability density depends on the choice% of units for X. If you replot X using different units, you will change% the scale of the pdf axis!!! This also tells you that the probability% densities do not, generally speaking, sum to one, unlike the discrete% probabilities, which always sum to 1 (if you have included all possible% outcomes in the sum). The probability densities do not sum to 1 because% they are not probabilities; they are rates at which probability is being% gained (by the cumulative probability function). However, PROBABILITY% DENSITY FUNCTIONS ALWAYS INTEGRATE TO 1. This statement is the equivalent% to the statement that discrete probability distributions always sum to 1.% Both statements reflect the elementary fact that the observed outcome% (datum) must be among the possible outcomes.%% Example of effect of changing scale on pdffigure % Opens another figure window [Figure(6)]subplot(2,1,1);yyaxis leftplot(X,Y);hold onxlabel('X'), ylabel('Probability density');ax = axis;% plotting a vertical dashed line at the peak of the pdfline([.7 .7],[ax(3) ax(4)],'Color','k','LineStyle','--');yyaxis rightplot(X,normcdf(X,0.7,0.1));ylabel('Cumulative probability');title('Same as previous')subplot(2,1,2)yyaxis leftplot(10*X,normpdf(10*X,7,1));hold onxlabel('X'), ylabel('Probability density');ax = axis;% plotting a vertical dashed line at the peak of the pdfline([7 7],[ax(3) ax(4)],'Color','k','LineStyle','--');yyaxis rightplot(10*X,normcdf(10*X,7,1));ylabel('Cumulative probability');% Same plot as above, but after rescaling X (changing the units) by a% factor of 10. This will reduce the probability densities by a factor of% 10 even though we are plotting the same distribution as before.title('Units of X changed by a factor of 10')%% Gamma Distribution: % The distribution of the intervals required for a random rate process to% generate N events when the expected interval between events is I.% The expected interval between events is 1/labmda, where lambda is the% number of events per unit of time (the rate at which the random events% are happening)T = (0:0.1:5)'; % Vector of (some likely) times. Of course, because this is a continuous% distribution (because there are an infinite number of possible% durations), we will be plotting probability densities not probabilities.% A probability density is associated with every possible value on the X% axis. A probability density function gives for each possible value of x% the associated probability density.figure % Figure(7)subplot(2,1,1)plot(T,gampdf(T,1,0.5));hold on % The gamma distribution is evaluated at the intervals in the vector T. The% distribution has parameters 1 and 0.5. The interpretation of these% parameters varies from context to context, but in the present context, 1% is the required count, and .5 is the expected interval between the events% being counted, that is, lambda is 2 (events per unit of time). The% expected interval between events is 1/rate.plot(T,gampdf(T,2,0.5),'r') % Superimposing the distribution when the count is 2, that is, we are% plotting the intervals it takes to generate a count of 2plot(T,gampdf(T,3,0.5),'g') % Distribution when required count is 3plot(T,gampdf(T,4,0.5),'k') % Distribution when required count is 4legend('N=1','N=2','N=3','N=4') % N is the required counttitle('I constant at 0.5 (i.e., lambda=2), N varying')ylabel('Probability Density')xlabel('Time')% The gamma function with N=1 is the exponential. In other words, the% exponential distribution is a special case of the gamma distributionsubplot(2,1,2)plot(T,gampdf(T,2,0.25));hold onplot(T,gampdf(T,2,0.5),'r')plot(T,gampdf(T,2,0.75),'g')plot(T,gampdf(T,2,1),'k')title('N constant at 2, lambda varying')legend('lambda=4, so I=0.25','lambda=2, so I=0.5','lambda=1.33, so I=0.75','lambda=1, so I=1.0')xlabel('Time');ylabel('Probability Density')%% Likelihood and likelihood functions% In model-fitting and parameter-estimation contexts, the values given by% pdf functions are likelihoods. Whereas an individual probability density% has a meaning (and units), an individual likelihood has no meaning (and% no units). The probability density associated with a given value of x is% the slope of the cumulative probability function at that value of x; in% other words, it is the rate at which the cumulative probability% distribution is gaining probability. Thus, whatever the units of x are,% the units of probability density are one over those units (those units to% the power of minus 1). Moreover, and critically, A PROBABILITY DENSITY% FUNCTION ALWAYS INTEGRATES TO ONE, reflecting the elementary truth that% the value x has must be somewhere among the values it could possibly% have.% By contrast, a likelihood has no meaning by itself; it has meaning only% as part of a likelihood function, and, generally speaking, LIKELIHOOD% FUNCTIONS DO NOT INTEGRATE TO ONE!% When a probability density function (pdf) is used as such, then it is% assumed to describe the probabiliity density (or, if it is a discrete% pdf, then the probability mass) associated with each possible value of x% (that is, whatever variable is on the x axis).% When a pdf function is used to get relative likelihoods, then the value% of x is assumed to be known (it is usually a datum) and the pdf is used% to obtain the relative likelihoods of possible values for the parameters% of a "model". (The "model" may simply be that the data come from a% distribution whose parameters one is trying to estimate.) As one changes% the assumed values of the parameters the graph of the resulting% ("predicted") pdf moves around on the x axis and changes its shape. As it% does so, the likelihood associated with the known x changes. This enables% one to compare the relative likelihoods of having observed that x, given% different assumptions about the values of the model parameters. For% example, consider two different values of the mean (or, more generally,% the location parameter of a distribution). These two different values% position the pdf at two different locations. The different locations for% the distribution give different likelihoods for that x value. It is the% ratio of these two likelihoods that is the meaningful quantity, not the% likelihoods themselves. The ratio gives the relative likelihood of having% observed that value x. If the likelihood of x when the distribution is at% Location 1 is 1.2 and the likelihood of x when the distribution is at% Location 2 is 0.3, then Location 1 is 4 times more likely (1.2/.3) than% Location 2 (4 times as likely to be the true location of the% distribution). In practice, there is always more than one x (more than% one datum--usually many. As one moves the pdf around (and/or changes its% shape), the likelihoods for some x's go up, while others go down. One% wants to find the locations and shapes that maximize the joint% likelihood, which is to say the product of the likelihoods (one% likelihood for each datum). Maximizing the product of the likelihoods is% the same as maximizing the sum of their logarithms, because taking% logarithms converts multiplication to addition. The 'mle' function in the% Statistics Toolbox varies the parameters of a model to find the values% that maximize log likelihood, that is, the sum of the logarithms of the% likelihoods. For each combination of values for the parameters of the% model, it uses the relevant pdf to find the likelihood associated with% each datum; it sums the logarithms of those likelihoods to get the% overall log likelihood, that is, the log likelihood for the whole data% set. It repeats this for many different possible parameter values and, in% this way, it finds by brute force the combination of parameter values% that maximizes the log likelihood of the data. (Actually, the brute force% is aided by some subtle tricks.)%% Computing likelihood functions: binomial% ANY(!) distribution function (discrete OR continuous) can be used to% compute a likelihood function. In using it that way, the datum (outcome)% is given and one uses the distribution function to compute how likely% that outcome is for various values of the parameters of the distribution% function. Thus, when the distribution function is used to compute% probabilities or probability densities, the parameter(s) of the% distribution is/are assumed to be given and one computes the probability% of the different possible outcomes (or the probability densities at the% various possible values of the outcome). When the same function is used% to compute likelihoods, the outcome is assumed to be given and one% computes the (relative) likelihoods of obtaining that outcome as the% value(s) of the distribution vary.p=0:0.02:1; % a vector of possible values of the p parameter of the binomial% distribution. Of course, there are an uncountable infinity of possible% values for this parameterL0=binopdf(0,5,p);figuresubplot(3,1,1)plot(p,L0)xlabel('P(heads)');ylabel('Likelihood')title('Likelihood of 0 heads in 5 flips as a function of P(heads)')% You can see at a glance that this function does not integrate to 1.% Indeed, likelihood functions are really only defined up to a% multiplicative constant (scaling factor), which is a way of saying that% the only interest in a likelihood function is the relative likelihoods of% the different possible values of the p parameter. Obviously, the outcome% of 0 heads in 5 flips is very much more likely if the probability of% heads is very small. From the likelihood function, we can determine the% value of p for which that outcome is only half as likely: draw a% horizontal line across the graph at .5 and drop the point of its% intersection with the likelihood function to the p axis.%% Computing likelihood functions: geometric% In the second example, we have observed that there were 5 flips before% the first heads. We compute the likelihood of producing this outcome for% various values of pL5=geopdf(5,p);subplot(3,1,2);plot(p,L5);xlabel('P(heads)');ylabel('Likelihood')title('Likelihood of 5 flips before the first head as a function of p(heads)')% Again, we can see at a glance that this function does not integrate to 1% (the area of the entire panel is equal only to .07 and the area under the% curve is a fraction of that area)%% Computing likelihood functions: Poisson% In the third example, we have counted the clicks from a Geiger counter% for 1 minute and obtained a count of 3. We compute the likelihood of% various possible true rates (counts per minute) in the light of this% observationLambda=0:0.25:20;% A vector of possible rates. Of course there are an uncountable infinity% of possible rates. This vector simply samples reasonably densely over the% range of plausible rates (plausible given that we got a count of only 3% in our single observation)L3=poisspdf(3,Lambda);subplot(3,1,3);plot(Lambda,L3)xlabel('True rates (counts per minute)');ylabel('Likelihood')title('Likelihood of a count of 3 as a function of the true rate')% As usual, the function does not integrate to 1%% Computing likelihood functions: normal% In the fourth example, we have observed the height of one pygmy to be 1.1% m. We compute the likelihood of observing this height under various% hypotheses about the mean and standard deviation of the population of% pygmy heights, which we assume to form a normal distributionH=0:0.1:5; % Vector of possible heights (the range that it would be plausible to% assume in the light of only this one observation and absent any auxilary% information (and priors) can be a subject of lengthy discussion, but here% we just plunge ahead. In creating this vector, we are choosing to look at% the range of heights from 0 to 5 meters in 10 cm (.1 m) increments. We% must be mindful of the possible effects of this choice on what we see in% the graph.Sigma=0.1:0.1:2; % Vector of possible standard deviation; again what is a plausible range a% priori is an interesting question for discussion. It would lead deeply in% to Bayesian approaches and why they stir so much controversy. Here, we% are choosing to look at values of Sigma ranging from 10 cm to 2 meters in% 10 cm increments. Again, we must be mindful of the possible effects of% this choice on what we see when we make the graphs[X3d,Y3d]=meshgrid(H,Sigma); % The likelihood function in this case will give the likelihood as a% function of the possible values of a pair of parameters, the possible% values of the mean (H) and the possible values of the standard deviation% (Sigma). Thus, the plot will be a 3D graph, rather than the 2D graphs we% have so far worked with. The 3D graph will show the likelihood (on the z% axis) for all possible combinations of mean and standard deviation. (Of% course, there are an uncountably infinite number of possible% combinations, so in practice, we must make the plot for a finite set of% possible combinations. The possible combinations of two different vectors% (two different "dimensions") may be thought of as the points in a plane,% whose dimensions are the mean (one dimension) and the standard deviation% (the other dimension of the plane of possible combinations). The plot% will show for every (selected) point in this plane (that is, for every% selected possible combination) the likelihood of that combination. The% likelihood of a given combination is represented by a point that hovers% "above" that combination: the farther above, the greater the likelihood.% The cloud of these hovering likelihood points form a surface, called the% likelihood surface. MATLAB will compute and plot this surface, but in% order to get it to do this, we have to provide it with all possible% combinations of the values that we have selected along each of our two% parameter dimensions. In order to make the computation, it must consider% for each value of the mean all possible values of the standard deviation% and vice versa. The required input arrays are computed by the 'meshgrid'% command. It takes the 51 values that we have selected along the mean% dimension and replicates that vector 20 times, making an X3d array that% has 20 rows and 51 columns. It takes the 20 values that we have selected% along the sigma dimension and replicates them 51 times, making a Y3d% array 0f 20 rows and 51 columns. If you think about laying the Y3d array% on top of the X3d array, you will realize that the resulting sandwich% pairs every selected value of H with every selected value of Sigma. We% now feed these two arrays to the 'normpdf' command. The first argument of% the normpdf function is the observed height of 1.1 meter, which is a% "scalar"(that is a single number, as opposed to a vector or an array).% The second and third arguments of the function, however, will now be X3d% and Y3d, which are the two just described arrays. The function returns an% array of the same size as these two. It contains the likelihood for each% combination of H and Sigma:Z = normpdf(1.1,X3d,Y3d);figure;mesh(X3d,Y3d,Z); % creates the 3D plotxlabel('Height (m)'); ylabel('Variability (sigma)'); zlabel('Likelihood');title('Likelihood function for height population parameters')% 3D plots take some getting used to. It is critical to view them from% different perspectives. To do that, look in the row of icons at the top% of the figure window and click on the icon with an arrow encircling a% cube. Now you can grab the figure with the cursor and drag it around into% any perspective you like. Try it, you'll like it. Notice among other% things that if the variation in pygmy height is as great as 1 meter (and% who's to say it isn't if we are truly agnostic about this--have no% priors--and we have only the one observation), then there is substantial% likelihood that the mean pygmy height is 0. This, of course is% preposterous. It means that we have not thought sufficiently about the% priors, but that is a longer story.%% Combining likeihoods from multiple measurements% In probabilistic reasoning, likelihoods refer to the the possible% "hypotheses" by which we might explain (or, at least, compactly% summarize) our data, and these hypotheses (or models) take the form of% distributions from which we hypothsize or conjecture that our% observations (data) have been drawn. There are two aspects to the% specification of an hypothesized distribution: 1) the conjectured form of% the distribution; 2) the values of the parameter(s) of the distribution% (e.g., the p parameter of the binomial and geometric distributions or the% mean and sigma parameters of the normal distribution). The form that we% suppose is usually suggested by a consideration of plausible forms given% what we think we understand about the process that generated the data,% although it is not uncommon to entertain different possible forms% (particularly when using regression, that is, when predicting the value% of one variable from the value(s) of other variables). The values for the% parameter(s) are usually estimated from the data. In doing this, we find% the parameter values that maximize the likelihood of the hypothesized% distribution, given the data. Given an assumed form and assumed value(s)% for the parameter(s), we get for each datum a likelihood. To obtain the% combined likelihood of the distribution, given all the data, we need a% rule for combining the individual likelihoods. The rule is% multiplication, that is, the likelihood of a distribution given two data% is the product of the likelihoods of that distribution given each datum% alone. The result is a likelihood function for the assumed distribution% form given the observed data. To get a feel for this, assume we obtain% the height of a second pygmy, and that her height is 1.16 m. So now we% have two observed heights: 1.10 and 1.16. We continue to assume the% heights are normally distributed, and we compute the likelihood function% for the parameters of this distribution, given the two heights we have% observed:Z2=normpdf(1.1,X3d,Y3d).*normpdf(1.16,X3d,Y3d);% Here we use the .* operator in MATLAB, which performs an element by% element multiplication of two vectors or arrays that have exactly the% same dimensions (the same number of rows and the same number of columns).% Bear in mind that normpdf(1.1,X3d,Y3d) generates an array that has the% same dimensions as the X3d and Y3d arrays. To visualize this, imagine the% Y3d laid on top of the X3d array, so that every cell in the X3d array is% paired with a cell in the Y3d array. (If the arrays do not have the same% dimensions, this pairing will fail.) The value in a cell of the X3d array% is a possible value of height, and the value in a cell of the Y3d is a% possible value of sigma (variability in height). When we lay one array on% top of the other, we pair all (of our selected) values of height with% (all of our selected) values of sigma. Now imagine, the output array laid% on top of the two input arrays: the value in a cell of the output array% is the likelihood of our datum given the values for height and sigma in% the corresponding (underlying) cells of the X3d and Y3d arrays. In the% present command, we perform this computation twice, once for the datum% (observed height) 1.1 (to left of .* operator) and once for the datum% 1.16 (to right of .* operator), so that gives us two arrays of% likelihoods. The .* operator lays those two arrays of likelihoods one% atop the other and multiplies them element by element (cell by% corresponding cell). In other words, that one short line of code causes% an astonishing amount of computation, the result of which is the% likelihood function given the two observed heights, which function we now% plotfiguremesh(X3d,Y3d,Z2) % creates the 3D plotxlabel('Height (m)'); ylabel('Variability (sigma)'); zlabel('Likelihood');title({'Likelihood function for height population parameters';...    'given observed heights of 1.1 & 1.16 m. Version 1'});%% Better starting estimates for parameters% The resulting graph is misleading, because we continued to use the% selected values for our H and sigma vectors that we used for our first% plot. In choosing those values, I wanted to show that the function spread% over a wide range. Now, with two observations, the range of possible% values for the height and sigma parameters with any appreciable% likelihood is much smaller. Worse, however, is the fact that the% likelihood function still appears to be headed off to infinity as the% value of sigma approaches 0, but this appearance is an artifact of the% fact that the lowest value for sigma that I chose to compute and graph is% 0.1. Experience (and thought) teaches that if I looked at still lower% values, the function would decline, as we now show by choosing% a more appropriate selection of values for H and sigma:% We now look at heights from 0.4 m to 1.4 m in 2 cm incrementsH=0.4:0.02:1.4;% We now look at variability estimates ranging from 0.5 cm to 0.4 meter (40% cm) in 1 cm incrementsSigma=0.005:0.01:0.4;% Re-do our calculation with new parameter ranges:[X3d,Y3d]=meshgrid(H,Sigma);Z2=normpdf(1.1,X3d,Y3d) .* normpdf(1.16,X3d,Y3d);figuresubplot(2,1,1);mesh(X3d,Y3d,Z2) % creates the 3D plotxlabel('Height (m)'); ylabel('Variability (sigma)');zlabel('Likelihood');title({'Likelihood function for pygmy height population parameters';...    'given observed heights of 1.1 & 1.16 m. Version 2'})% The contour function will give likelihood limits for the estimates of the% population mean height and variability. The contour function draws level% curves for surfaces like the one we see in the 3D plot of the likelihood% function. The likelihood function has the shape of a hill. Imagine a% flood in which the water rises to a certain level and leaves a line% around the hill at that level. The line is a constant-elevation curve or% level curve of the likelihood function (surface). The contour function% allows us to choose the levels at which we draw these curves. If we% choose levels equal to .05, .1 and .5 of the peak (maximum likelihood),% then these curves have the following interpretation. Any combination of% mean and sigma lying outside the .05 contour is at least 20 times less% likely to have produced our data than the best combination (the maximum% likelihood values of mean and sigma). Similarly, any combination lying% outside the .1 contour is at least 10 times less likely. These limits are% closely analogous to the confidence limits around parameter estimates in% more traditional statistics.subplot(2,1,2);% Make the contour plot[C,H]=contour(X3d(1:25,30:46),Y3d(1:25,30:46),Z2(1:25,30:46),...    [0.05*max(Z2(:)) 0.1*max(Z2(:)) 0.5*max(Z2(:))]);% Some trial and error revealed that the liklihood limits lay within the% rows 1:25 and columns 30:46 of the arrays being plotted (X3d, Y3d & Z2).% The levels are specified in the 4th argument, which gives the elevation% at the top of the hill around which we want to plot contours (the maximum% likelihood, that is, the maximum value of the likelihood function). By% multiplying this maximum elevation by .05, .1 and .5, we get elevations% at those fractions of the peak. Three curves will be drawn, one at each% of those elevations. The C and the H variables will be needed in order to% label these contoursHc = clabel(C,H,'manual');% This command activates an interactive function that allows us to position% the cursor over a contour and click on it so as to get an elevation level% for that contour. (The level is written on the contour at the location% where we click). Clicking on the three contours labels them. However, it% labels them with their absolute elevations (likelihood levels) and the% actual values of a likelihood function are meaningless. It is only% relative values that have meaning--how much more or less likely one% combination of parameters is than another. When done clicking, hit the% 'return' key while the graph is the active window. The Hc variable is a% handle that allows us to change the text in the labels of the contours:set(Hc(1),'String','0.5');set(Hc(2),'String','0.1');set(Hc(3),'String','0.05');xlabel('mean Height'); ylabel('Sigma');title('Likelihood Limits on Mean Height and Sigma of Pygmy Population');%% Using dfittool% The dfittool function in MATLAB is an interactive function that you can% use to find the maximum likelihood estimates for the parameters of a% variety of distributions. It has the further advantage that it gives you% the maximum log likelihood, which is an essential quantity in comparing% the relative likelihoods of different models for your data. We have just% seen the logic underlying what it does: it finds the values for the% parameters of the chosen distribution that maximize the likelihood% function given your data (the values that give the peak likelihood). We% begin by generating some simulated data from a Poisson distribution, with% expectation 1.85rng default;    % for reproducibilityData = poissrnd(1.85,50,1);dfittool(Data);% Opens the 'dfittool' interactive dialog box and imports our data. When it% opens, the dfittool window will show a histogram of our data. Clicking on% the New Fit button brings up another dialog box continaing a pull-down% menu with a list of distributions we might try to fit to our data. We% select the Poisson distribution first and click the Apply button. Now the% Results box at the bottom of this dialog box gives the maximum likelihood% estimate of the lambda (expectation) parameter AND it gives the log% likelihood value associated with this particular distribution, that is,% the Poisson distribution with this value for lambda. This log likelihood% estimate can be used to compare the likelihood of this model for our data% with the likelihood of other models. It also plots the best fitting% distribution on top of the histogram of our data, allowing us to see how% well it describes that data. Notice that it plots it as a line plot% (shame on it). It redeems itself, however, if we use the Display type% pull-down menu in the window with the plots to select the cdf display% rather than the pdf. Now, it plots both our data and the fit as step% (stairs) plots, which is how they should be plotted.% We next try a normal distribution as a model for our data. This% is more than a little stupid, since our data are integers, so we are% dealing with a discrete distribution. The Poisson distribution% is a discrete distribution but the normal distribution is a continuous% distribution. A priori, the normal distribution is implausible.% Nonetheless, the literature is full of analyses based on implicitly% normal models for data that are in fact discrete, so let's follow the% herd. To try a different model, we click on the Manage Fits button, which% allows us to choose again from the list of distributions. We choose% Normal this time and click Apply. Now, we get the maximum likelihood% estimates for the mean and sigma of the normal distribution and the log% likelihood associated with these estimates. We also see the new fit% plotted along with our data and the previous fit. If we have the selected% the cdf form for these plots, the new fit is plotted as a continuous% function, as it should be, whereas our data and the previous fit are% plotted as stair functions, as they should be.% The log likelihood for the Poisson fit is -82.4872, while the log% likelihood for the normal fit is -84.5159. The difference in these log% likelihoods is 2.0287, which means that the Poisson model is at least% exp(2.03) = roughly 7.6 times more likely than the normal model even% though it is a simpler model; it only has one parameter, whereas the% normal model has two. A proper comparison of the relative likelihood of% these two models would penalize the normal model for its additional% complexity. We will see how to do that when we get to the Akaike% Information Criterion. In any event, a proper comparison of the relative% likelihoods of the two models would reveal the Poisson model to be% substantially more than 7 times as likely as the normal model.%% F distribution and ANOVA% The distribution of the ratio of two variance estimates% Analysis of variance is fundamentally about estimating in two different% ways what would be the same variance if the null hypothesis were true,% and then looking at the ratio of those estimates. On the null hypothesis,% the expected value of this ratio is 1, that is, the two estimates of% variance should agree. The fcdf gives the cumulative distribution of% these estimates for a given pair of degrees of freedom. From it, you read% off 1-p, the probability of obtaining an F ratio smaller than the one you% obtainedfigure % Figure(14)F=(1.5:0.25:10)'; % Vector of values for Fplot(F,fcdf(F,1,10)); hold on% Cumulative distribution of the F statistic (the ratio between two% estimates of variance) when one estimate is based on only 2 observations% (df=1) and the other on 11 observations (df = 10). Typically, in an ANOVA% context, the variance estimate in the numerator would be based on the% sample means, while the variance estimate in the denominator would be% based on the variance within the cells (so-called error variance)plot(F,fcdf(F,1,100),'--') % df in denominator increased to 100plot(F,fcdf(F,2,10),'r') % df in numerator increased to 2plot(F,fcdf(F,2,100),'--r') % effect of the two increases combinedplot([1 10],[.95 .95],'-.k') % horizontal dashed line at a probability of .95plot([1 10],[.99 .99],'-.k') % horizontal dashed line at a probability of .99legend('df1=1, df2=10','df1=1, df2=100','df1=2, df2=10','df1=2, df2=100',...    'Location','SouthEast');title('Cumulative F Distributions');xlabel('F Value'); ylabel('Probability of an F less than That')% The t statistic is the difference between the two means divided by the% estimate of the standard deviation in this difference. This statistic is% distributed according to the t distribution. You use tcdf to find the% value of 1-p corresponding to the t value you have computed% Similarly, the X^2 statistic is the sum of the squared deviations between% the observed and expected frequencies in a chi square table. You use the% chi2cdf function to find the 1-p value for your chi square statistic.% These examples show you the role of the cdf functions in determining the% probabilities associated with hypothesis-testing statistics, like F, t,% X^2, and so on.% Standard tests of the null hypothesis%% ANOVA% The commands that follow generate some data for us to analyze. In% following this, imagine an 2x3 ANOVA data table, a table with two rows% and three columns, each cell of which contains 15 observations (measures,% data). The rows represent levels of Factor 1; the columns, levels of% Factor 2 (For factor, you can also read "Treatment"; for Level% "Condition".) Thus, there are two levels of Factor 1 (e.g., "Drug" and% "No Drug') and three levels of Factor 2 (e.g., "Young" "Middle-Aged",% "Elderly"). An observation (datum, measure of some kind) is indexed by% the combination of levels. The first digit indicates the level of Factor% 1; the second, the level of Factor 2. Thus, 23, indicates that that datum% came from Level 2 of Factor 1 ("No Drug") and Level 3 of Factor 2% ("Elderly")F11=2*randn(15,1)+10; % A column vector 15 entries long comprised of random samples from a normal% distribution with a mean (mu) of 10 and a standard deviation (sigma) of% 2. The randn function delivers random samples from a normal distribution% with a mean of 0 and a standard deviation of 1. To make them into random% samples from a distribution with mean mu and standard deviation sigma,% you multiply (scale) the samples by sigma and add mu. These are the data% for the upper left cell in the ANOVA data table. Alternatively, one could% use normrnd(mu,sigma,R,C), which generates an RxC array of values drawn% from a normal distribution with mean mu and standard deviation sigma.F11=[F11 ones(size(F11)) ones(size(F11)) 11*ones(size(F11))]; % We add to the original single column vector three more columns, which% serve to specify groupings that we will need later in the analysis. The% first column identifies the level of Factor 1 (the row factor). A 1 in% this column means that the observation was made at Level 1 of Factor 1.% The second column identifies the level of Factor 2; a 1 here means that% the observation was made at Level 1 of Factor 2. The third column% identifies the cell, that is, the combination of levels. A 11 here means% that the observation was made at Level 1 of Factor 1 and Level 1 of% Factor 2. This column is, of course, redundant: we can deduce its% contents from the numbers in the first two columns; but it will be% convenient later on to have this column.F12=2*rand(15,1)+11.6;% A column vector 15 entries long, comprised of random samples from a% distribution with mu 11.6 and sigma 2F12=[F12 ones(size(F12)) 2*ones(size(F12)) 12*ones(size(F12))]; % Puting% adding the index values (group identifiers): 1 2 12F13=2*rand(15,1)+13.8; % mu = 13.8, sigma = 2F13=[F13 ones(size(F13)) 3*ones(size(F13)) 13*ones(size(F13))]; % 1 3 13F21=2*randn(15,1)+15; % mu=15, sigma = 2F21=[F21 2*ones(size(F21)) ones(size(F21)) 21*ones(size(F21))]; % 2 1 21F22=2*randn(15,1)+14.5; % mu = 14.5, sigma = 2F22=[F22 2*ones(size(F22)) 2*ones(size(F22)) 22*ones(size(F22))]; % 2 2 22F23=2*randn(15,1)+14; % mu = 14, sigma=2F23=[F23 2*ones(size(F23)) 3*ones(size(F23)) 23*ones(size(F23))]; % 2 3 23Data = [F11;F12;F13;F21;F22;F23];% Our data table. Data are in the first column; group identifiers (indices)% are in the next three columns%% Using 'anova1' to analyze first level of Factor 1:% that is, the effect of age (levels of Factor 2) on subjects receiving the% drug (one level of Factor 1, the drug treatment factor)figure % Opens another figure window[Means1,SEM1,Counts1,Gname] = grpstats(Data(1:45,1),Data(1:45,3),0.05);% grpstats is used to slice the data table in various ways and look at the% data graphically to see what is going on. Those who do an ANOVA without% first studying the data carefully by graphic means deserve their fate,% which is to screw things up. By the time you run the full ANOVA you% should have a clear idea of what it will reveal from having studied the% table graphically slice by slice. Here, we study the effect of age on the% subjects receiving the drug. The first argument of grpstats,% Data(1:45,1), is the vector containing the data; the second, Data(1:45,2), % is the vector distinguishing the groups. The third argument, 0.05, tells% it that we want to see a graph of the means, with the 95% confidence% intervals about those means. If you omit this argument, you don't get a% graph. The variables on the left of the equals sign will get the results:% Means1 will be a vector containing the three means; SEM1, a vector% containing the three standard errors, Counts1 a vector containing the% three n's, and Gname, a vector containing the indices that distinguish% the groups. Instead of numerical indices, we could use text variables to% distinguish groups (e.g., 'Young', Middle-Aged','Elderly') but this is% somewhat awkward because of MATLAB's resistance to combining numerical% and text data in a single table. (You can do it with cell arrays, but% then you cannot easily get a display of the text and numbers together.)hold on % We want to plot the data from the "No Drug" conditions on the same graph% as the data from the "Drug" conditions[Means2,SEM2,Counts2,Gname2] = grpstats(Data(46:90,1),Data(46:90,3),0.05);% Now we look at the effect of age in the groups that did not get the drug;% and we plot these effects on the same graph as we have already plotted% the effect of age in the "Drug" group--to see whether there is an% interaction between Age and Drug. There obviously is. Whatever it is we% measured, it depends strongly on age when subjects have been treated with% the drug, but not when they are undrugged.legend('Drug','No Drug');xlabel('Age');set(gca,'XTickLabel',{'Young','Middle-aged','Elderly'});%% Using anova1, which does a one-way analysis of variance.anova1(Data(1:45,1),Data(1:45,3)); % One way ANOVA on "Drug" groups, the lazy man's way. This command% generates two figure windows. One gives the usual ANOVA table, showing% the sums squares, dfs, F and p value. The other is a notched box plot. A% box plot has a line at the median, enclosed by a rectangle (the box),% whose upper and lower boundaries are at the upper and lower quartiles. A% notched box has a notch surrounding the median, the limits of which give% robust estimates of the 95% confidence interval about the median. This% gives you an eyeball non-parametric test of whether the effects seen in% the graph are statistically significant. Boxes whose notches do not% overlap indicate that the medians of the two groups differ at the 5%% significance level. Whiskers extend from the box out to the most extreme% data value within WHIS*IQR, where WHIS is the value of the 'whisker'% parameter and IQR is the interquartile range of the sample. These% nonparametric tests are in addition to or alternative to the P value% returned by anova1, which is, of course, a parametric statistic. Clearly% age matters, at least, under this drug condition. The first argument of% anova1, Data(1:45,1), gives the vector containing the data; the second,% Data(1:45,3), gives the indices that distinguish between groups, in this% case, the age indices, which are in column 3 of our data table (array).% If your data are in the form of a matrix, with each column containing the% data from a different condition, then you can just call anova1(M). It% will assume that each column is a group. The more complicated call to% anova1 used here, in which a second argument indicates which observations% belong to which conditions is only really necessary when there are% unequal n's.%% ANOVA with understanding (from first principles)Var11=var(Data(1:15,1)); % within group variancesVar12=var(Data(16:30,1));Var13=var(Data(31:45,1));Var1group = (Var11+Var12+Var13)/3; % Mean within-group variance for the cells at Level 1 of Factor 1BtwnGroupVar1 = 15*var(Means1); % On the null hypothesis, the variance between the sample means,% var(Means1), should be 15 times smaller than the variance within the% samples, because the fundamental mathematical result that underlies% analysis of variance is that var(Means) = var(sample)/n where n is the% sample size. This is the law of large numbers, why it's good to have% large samples: the larger the samples, the less variable the resulting% means.F = BtwnGroupVar1/Var1group;% The F value generated by this computation should be the same as the value% in the ANOVA table generated by anova1%% 2-way ANOVA with 'anova2'% which does a 2-way analysis of variance when the cells are balanced (have% equal n's)Data2 = [Data(1:45,1) Data(46:90,1)]; % The two way anova function assumes that each column is a level of Factor% 1 and that the levels of Factor 2 are contained in multiples of some% number of rows (in this case 15)[~,Table] = anova2(Data2,15);% the p values for the effects of the factors and their interaction are% returned to the variable P (on left of equals); the variable T contains% the cell array showing the ANOVA table. If you see 00 p values, it means% that they are less than .0005. In the ANOVA table, "Columns" refers to% Factor 1 ("Drug", "No Drug"),and "Rows" refers to Age.%% Using ttest2 to compare two samples[H,P,CI,Stats] = ttest2(Data(31:45,1),Data(76:90,1),0.05,'both');disp(P);% 2-tailed t test; H is 0 if the null hypothesis cannot be rejected at the% specified alpha level and 1 if it can; P is the p-value associated with% the value of the t statistic; CI is the 95% confidence interval for the% true mean of the difference (aka true difference of the means).%% Using 'anovan' for more complex ANOVA% including all "unbalanced" ANOVAs, that is, ANOVAs with unequal numbers% of observations in the cells. The anovan function is a general-purpose% anova-producing function. It can handle anovas of great% complexity--greater than the user has any hope of understanding. Here we% use it simply to do an unbalanced 2-way ANOVADataUn=Data;    % copy our original data setDataUn([27:30 40:45 59:60 69:75 86:90],:)=[];% Unequal numbers of observations in the cells (=unbalanced)% Finding rows corresponding to different treatmentsDrgR=find(DataUn(:,2)==1);NoDrgR=find(DataUn(:,2)==2);figure[Means1un,SEM1un,Counts1un] = grpstats(DataUn(DrgR,1),DataUn(DrgR,3),0.05);% Those who do not first examine their data graphically deserve their fatehold on[Means2un,SEM2un,Counts2un] = grpstats(DataUn(NoDrgR,1),DataUn(NoDrgR,3),.05);title('Means & 95% Confidence Intervals for Drug & NoDrug Conditions');xlabel ('Age');set(gca,'XTickLabel',{'Young','Middle-aged','Elderly'});legend('Drug','No Drug');% We see again that there is an interaction: age matters only when subjects% are drugged. Now we do the ANOVA (just to keep the editors happy)Lvls{1,1}='Drg/NoDrg';Lvls{1,2}='Age'; % 'anovan' requires that the indices identifying groups be in cells. We're% going to put the indices for the Drug/NoDrug group in one cell and the% indices for age in the other. While we're at it, we might as well use the% cells above these to tell us what these indices refer to. Because we're% going to have to use a cell array anyway, we could convert the indices to% text, e.g. 1->'Drug', 2->'NoDg', which would make the results more% readily intelligible.% Putting the indices into the cellsGrp1 = cell(size(DataUn(:,2)));Grp1(DataUn(:,2) == 1) = {'Drg'};Grp1(DataUn(:,2) == 2) = {'NoDrg'};Grp2 = cell(size(DataUn(:,3)));Grp2(DataUn(:,3) == 1) = {'Young'};Grp2(DataUn(:,3) == 2) = {'Middle-aged'};Grp2(DataUn(:,3) == 3) = {'Elderly'};[P,Tbl] = anovan(DataUn(:,1),{Grp1,Grp2},'model','interaction',...    'varnames',{'Drug','Age'});% P contains the vector of p-values for the two main effects and the% interaction; Tbl contains the cell array with the ANOVA table (SSs, MSs,% dfs, Fs, ps). Because there is an interaction, the main effects should% not be believed until we have tested them in isolation. Because we% examined our data graphically beforehand we are not flying blind,% unlike most users of SPSS; we know what's going on: age only% matters in drugged subjects. We check this by running separate 1-way% ANOVAS for the Drug and No Drug conditions.[PDrg,TblDrg] = anova1(Data(DrgR,1),Data(DrgR,3));% The data are in the vector Data(DrgR,1); the indices distinguishing the% age groups are in the vector Data(DrgR,3). Recall that DrgR is a vector% of row numbers, identifying the rows in the data table that have data% from the drugged condition. We see that there is a highly significant% effect of age in the drugged condition.[PNoDrg,TblNoDrg] = anova1(Data(NoDrgR,1),Data(NoDrgR,3));% There may or may not be a significant effect of age in undrugged% subjects, depending on the vagaries of the random number generating% scheme. Often, the effect will be marginal.